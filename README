Latent Semantic Analysis Similarity Calculator
===
This container is used to perform Latent Semantic Analysis (LSA) on short 
texts. This includes the ability to create a semantic space as well as the 
ability to calculate cosine similarity scores on short texts using a semantic 
space.

Running the Tool
===
In order to run the tool, type the following into your terminal
~~~bash
docker run -t -v /local/data/path/:/app/data o76923/lsa
~~~
Where `/local/data/path/` is the path on your local system that contains the 
source information.

Directory Structure
===
The path on your local machine should contain the following
1. The semantic space
1. The texts to be compared
1. A configuration file

Semantic Space
===
The semantic space is the corpus that is used in order to create similarity 
files.

Texts to be Compared
===
The texts to be compared are the short texts that you wish to have compared to 
one another. Similarity scores will be generated between texts with one ID and
texts with another ID.

Configuration File
===
The configuration file specifies the parameters that will tweak how the tool 
behaves. The sections of it are as follows

Tasks
---
There are two main tasks that can be performed by this tool: "create_space" and
"calculate_sim". At the moment, each task may only be performed once in a 
configuration file, though the ability to have multiple copies of the same task
is something that will be implemented in the future.

***create_space***

This task takes input documents and creates a semantic space from them. A 
sample of this section is provided below
~~~yaml
  - type: create_space
    name: IS
    space_settings:
      stem: true
      case_insensitive: true
      dimensions: 300
      remove:
        - punctuation
        - singletons
        - numbers
        - stopwords:
            library: nltk
    from:
      document_scope: line
      files:
        - paragraph.txt
    output:
      su_mat: IS_su.npy
      vst_mat: IS_vs.npy
~~~
* **name** specifies the name of the newly created space that will be used when
referring to the space elsewhere and controls the directory that files will be
placed in for this space.
* **space_settings** specifies details about how LSA is performed
    * *stem* is either "true" or "false" depending on if words should be passed
    into the Porter Stemmer. Default is "true" where words are passed to the 
    stemmer.
    * *case_insensitive* is either "true" or "false" depending on whether 
    capitalization of words matters. Default is "true" where all words are 
    changed to lower case.
    * *dimensions* is an integer that specifies the rank of the reduced matrix.
    This corresponds to the number of most significant eigenvectors that are
    computed and stored. Default is "300".
    * *remove* is a list of options for what should be removed from the space
    during pre-processing. Any of the following present in thefile will be 
    removed options not present in the file will remain in the document. The 
    options are:
        * punctuation - characters not A-Z or a-z
        * singletons - words that appear only in a single document in the 
        corpus
        * numbers - the digits 0-9
        * stopwords - words that should be removed based on how common they
        are in language. The default option here is to specify "library: nltk"
        which says to use the corpus of stopwords from the NLTK corpus in 
        python
    * *accent* is either "remove_accent", "remove_letter", or "unchanged". This
    option determines how letters with accents are handled. With 
    "remove_accent" selected, letters are converted to a form of them without
    the accent. With "remove_letter" selected, any accented character is
    removed. With "unchanged", the accent is retained. The default is
    "remove_letter" and changing this default is not yet implemented.
* **from** specifies details about the documents that for the basis of the 
space.
    * *document_scope* is either "line" or "file" and specifies whether the
    documents in the semantic space should be per line or per file. If "line"
    is selected then each line of the loaded files represents a single document
    if "file" is selected than the entire contents of a file are considered
    to be the same document.
    * *files* is a list of file names. Each file is read, one at a time, and
    is broken into documents based on the document_scope setting.
    * *directories* is a list of directories. Each file with the .txt extension
    in a directory in this list will be read, one at a time. These files will 
    be processed based on the document_scope setting. This option is not yet
    implemented.
* **output** specifies options if you wish to have data from the semantic space
dumped to files for use in other tools. The two options you have are "su_mat"
and "vst_mat" which give you the SU or (VS)^T matrices from the semantic space.
These are stored in a binary format used by numpy in order to reduce size. This
feature is not yet implemented.

***calculate_sim***

This task reads in a collection of short texts to be compared and runs
cosine similarities comparing them. A sample of this section is provided below
~~~yaml
  - type: calculate_sims
    space: IS
    from:
      files:
        - MIDUSquestions.txt
        - CDEnames.txt
      pairs: cross
    output:
      filename: midus_cde_LSA_IS.csv
      format: csv
      header: false
      threshold: -10.0
      similarity_count:
        left: 10
        right: 0
    options:
      batch_size: 100
~~~
* **space** is the name of the semantic space that similarities should be
calculated in. This corresponds to the name given during the create_space task
and to the directory that it is stored in.
* **from** specifies the short texts to be compared and some options about how
they are compared.
    * *files* is a list of file names. Files are expected to contain an id 
    followed by the tab character followed by the text of the item to be 
    compared. While an arbitrary number of files can be specified, only the 
    first two are meaningfully distinguished within the code.
    * *pairs* specifies which items you would like compared and can be any one
    of "cross", "all", or a file name. If "all" is selected, then every 
    possible combination of items is included. If "cross" is selected then all
    items in the first file in the files section are compared to all items in 
    the second file in the files section. If a filename is provided it is
    expected to be a CSV that contains IDs of specific pairs you are interested
    in comparing. The filename option is not yet implemented.
* **output** specifies options related to the results that you want generated.
At the moment, none of these options have been implemented.
    * *filename* is the name of the similarity file that you want generated.
    The default is "sims.csv"
    * *format* is the format of the output file that you want results generated
    in. The default is "csv" and at present, that is the only option.
    * *header* is either "true" or "false" and specifies if there should be a
    header row indicating what each column represents. Alternatively, a list
    may be provided where the first item is the label for the left ID, the 
    second is the label for the right ID, and the third is the label for the
    similarity score.
    * *threshold* is a float typically between -1.0 and 1.0. Only pairs with a
    similarity score of at least this value are written to the output file. The
    default is -10.0 which will include all similarities even if there is a
    significant rounding error.
    * *min_similarities* specifies the minimum number of desired results for a
    given item. You may either specify an integer or options for "left" and 
    "right". If you specify an integer, at least that many pairs using each
    item will be included, starting with the highest similarity. If you are
    using the "cross" method for pairs then you can instead specify "left" and
    "right" separately.
* **options** provides the ability to pass a few other details to the task
of calculating similarities. These are used to tune the algorithm. Details
on their use will not be specified until better guidance is available.

Options
---
Options specifies global options that will apply to all tasks run.